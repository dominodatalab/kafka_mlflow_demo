{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DO IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import random\n",
    "from confluent_kafka import TopicPartition,Producer,Consumer\n",
    "import uuid\n",
    "import certifi\n",
    "import codecs\n",
    "import os\n",
    "\n",
    "import mlflow\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas\n",
    "import pickle\n",
    "\n",
    "CLUSTER_API_KEY= os.environ['KAFKA_USER_NAME']\n",
    "CLUSTER_API_SECRET= os.environ['KAFKA_PASSWORD']\n",
    "KAFKA_BOOTSTRAP_SERVERS = os.environ['KAFKA_BOOTSTRAP_SERVERS']\n",
    "os.environ['KAFKA_USER_NAME']=CLUSTER_API_KEY\n",
    "os.environ['KAFKA_PASSWORD']=CLUSTER_API_SECRET\n",
    "os.environ['KAFKA_BOOTSTRAP_SERVERS']='pkc-6ojv2.us-west4.gcp.confluent.cloud:9092'\n",
    "os.environ['KAFKA_FEATURES_TOPIC_PARTITION_RANGE']= '0-4' #Inclusive of 0 and 9\n",
    "model_name= os.environ['DOMINO_PROJECT_NAME']\n",
    "model_name_prefix = model_name\n",
    "uuid=uuid.uuid1()\n",
    "\n",
    "FEATURES_TOPIC=f'{model_name_prefix}-features'\n",
    "PREDICTION_TOPIC=f'{model_name_prefix}-predictions'\n",
    "MODEL_UPDATE_TOPIC=f'{model_name_prefix}-updates'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a test model and publish to MLflow Model Registry\n",
    "\n",
    "![Alt text](./images/publish_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## What is a Domino Model API Endpoint in the Streaming Context\n",
    "\n",
    "A Domino API Endpoint is a multi-relica REST Endpoint which also acts as a Kafka Consumer and Producer. It consumes features and model version updates \n",
    "from separate Kafka topics and generates predictions and writes them to the prediction topic. This mode of predictions operates on the push mechanism \n",
    "where the incoming features are passed to the latest model version and generates new predictions.\n",
    "\n",
    "It also provides an API Endpoint to invoke interactively. By default it uses the latest model but can also be used to generate perdiction of any previous model version.\n",
    "\n",
    "A Model Endpoint is configured with-\n",
    "1. A Python file containing the model code\n",
    "2. A function call to be invoked when invoking the model via REST call\n",
    "3. Environment variables which contain all the connectivity details for Kafka (These can be be also injected in more secure manner such as Vault or \n",
    "                                                                               IRSA or Workload Idenities)\n",
    "4. A Kafka Consumer or Producer that is continuously listening.\n",
    "\n",
    "\n",
    "The Kafka Consumers for features across all model api instances will use the same consumer group id. However the Kafka Consumer for the Model Updates topic will use\n",
    "a unique group id per instance.\n",
    "\n",
    "\n",
    "![Alt text](./images/make_rest_calls.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = model_name_prefix\n",
    "#EXPERIMENT_ID = mlflow.create_experiment(EXPERIMENT_NAME)\n",
    "exp = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "if not exp:\n",
    "    EXPERIMENT_ID = mlflow.create_experiment(EXPERIMENT_NAME)\n",
    "else:\n",
    "    EXPERIMENT_ID = exp.experiment_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waiting for more features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "Registered model 'KAFKA_DEMO' already exists. Creating a new version of this model...\n",
      "2023/03/01 20:52:34 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: KAFKA_DEMO, version 7\n",
      "Created version '7' of model 'KAFKA_DEMO'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1)\n",
    "idx=0\n",
    "RUN_NAME = f\"run_{idx}\"\n",
    "with mlflow.start_run(experiment_id=EXPERIMENT_ID, run_name=RUN_NAME) as run:\n",
    "    # Retrieve run id\n",
    "    RUN_ID = run.info.run_id\n",
    "    data = sklearn.datasets.make_classification(n_samples=1000, n_classes=2,n_clusters_per_class=1, n_features=5,n_informative=2, n_redundant=0, n_repeated=0)\n",
    "    X = data[0]\n",
    "    y = data[1]\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X, y)\n",
    "    # Track parameters\n",
    "    mlflow.log_param(\"n_classes\", 2)\n",
    "    mlflow.log_param(\"n_clusters_per_class\", 1)\n",
    "    mlflow.log_param(\"n_features\", 5)\n",
    "    mlflow.log_param(\"n_informative\", 2)\n",
    "    mlflow.log_param(\"n_redundant\", 0)\n",
    "    mlflow.log_param(\"n_repeated\", 0)\n",
    "\n",
    "    # Track metrics - Value is fake for this demo\n",
    "    accuracy = 90\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    \n",
    "\n",
    "    # Track model\n",
    "    output = mlflow.sklearn.log_model(model, \"regression\",registered_model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ModelVersion: creation_timestamp=1677703954257, current_stage='None', description='', last_updated_timestamp=1677703954257, name='KAFKA_DEMO', run_id='9ef9805273114b759702d36901d17b11', run_link='', source='mlflow-artifacts:/mlflow/9ef9805273114b759702d36901d17b11/artifacts/regression', status='READY', status_message='', tags={'mlflow.domino.environment_id': '63fa71e8e05a501c43a61ac6',\n",
      " 'mlflow.domino.environment_revision_id': '63fa71fde05a501c43a61ac9',\n",
      " 'mlflow.domino.hardware_tier': 'small-k8s',\n",
      " 'mlflow.domino.project': 'KAFKA_DEMO',\n",
      " 'mlflow.domino.project_id': '63fd1d2ee75fd03d43b53700',\n",
      " 'mlflow.domino.project_name': 'KAFKA_DEMO',\n",
      " 'mlflow.domino.run_id': '63ffaca757a6942c612b61cb',\n",
      " 'mlflow.domino.run_number': '7',\n",
      " 'mlflow.domino.user': 'integration-test'}, user_id='', version='7'>\n"
     ]
    }
   ],
   "source": [
    "#print(output.model_uri)\n",
    "client =  mlflow.tracking.MlflowClient()\n",
    "registered_model = client.get_registered_model(model_name)\n",
    "#print(registered_model)\n",
    "versions = registered_model.latest_versions\n",
    "print(versions[0])\n",
    "model_name = versions[0].name\n",
    "model_version = versions[0].version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ModelVersion: creation_timestamp=1677703954257, current_stage='Staging', description='', last_updated_timestamp=1677703954730, name='KAFKA_DEMO', run_id='9ef9805273114b759702d36901d17b11', run_link='', source='mlflow-artifacts:/mlflow/9ef9805273114b759702d36901d17b11/artifacts/regression', status='READY', status_message='', tags={'mlflow.domino.environment_id': '63fa71e8e05a501c43a61ac6',\n",
       " 'mlflow.domino.environment_revision_id': '63fa71fde05a501c43a61ac9',\n",
       " 'mlflow.domino.hardware_tier': 'small-k8s',\n",
       " 'mlflow.domino.project': 'KAFKA_DEMO',\n",
       " 'mlflow.domino.project_id': '63fd1d2ee75fd03d43b53700',\n",
       " 'mlflow.domino.project_name': 'KAFKA_DEMO',\n",
       " 'mlflow.domino.run_id': '63ffaca757a6942c612b61cb',\n",
       " 'mlflow.domino.run_number': '7',\n",
       " 'mlflow.domino.user': 'integration-test'}, user_id='', version='7'>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.transition_model_version_stage(\n",
    "  name=model_name,\n",
    "  version=model_version,\n",
    "  stage='Staging',\n",
    ")\n",
    "#client.get_registered_model(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.linear_model._logistic.LogisticRegression'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waiting for more features\n"
     ]
    }
   ],
   "source": [
    "#Test Model Artifact Serialization and Deserialization\n",
    "import codecs\n",
    "import pickle\n",
    "\n",
    "pickled = codecs.encode(pickle.dumps(model), \"base64\").decode()\n",
    "model_instance = pickle.loads(codecs.decode(pickled.encode(), \"base64\"))\n",
    "print(type(model_instance))\n",
    "model_instance.predict([[1,1,1,1,1]])[0]      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Publish Serialized Model To Kafka Topic\n",
    "\n",
    "**All Domino Model API Instances will have the same Kafka Consumer Group Id to ensure that messages are processed mostly once, and idempotently**\n",
    "\n",
    "![Alt text](./images/kafka_mlflow.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KAFKA_DEMO\n",
      "KAFKA_DEMO-updates\n",
      "Retrived7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting\n",
      "waiting for more features\n",
      "Waiting\n",
      "waiting for more features\n",
      "Waiting\n",
      "waiting for more features\n",
      "Waiting\n",
      "waiting for more features\n"
     ]
    }
   ],
   "source": [
    "# Import dependencies\n",
    "import os\n",
    "import random\n",
    "import certifi\n",
    "from confluent_kafka import TopicPartition,Producer,Consumer\n",
    "import json\n",
    "\n",
    "import uuid\n",
    "uuid=uuid.uuid1()\n",
    "version = model_version\n",
    "\n",
    "print(model_name)\n",
    "client_id = f'{model_name}-publish'\n",
    "producer_conf = {'bootstrap.servers': os.environ.get('KAFKA_BOOTSTRAP_SERVERS'),\n",
    "                 'sasl.username': os.environ.get('KAFKA_USER_NAME'),\n",
    "                 'sasl.password': os.environ.get('KAFKA_PASSWORD'),\n",
    "                 'sasl.mechanism': 'PLAIN',\n",
    "                 'security.protocol': 'SASL_SSL',\n",
    "                 'ssl.ca.location': certifi.where(),\n",
    "                 'client.id': client_id}\n",
    "\n",
    "MODEL_UPDATES_TOPIC=f'{model_name}-updates'\n",
    "print(MODEL_UPDATES_TOPIC)\n",
    "\n",
    "model_updates_producer = Producer(producer_conf)\n",
    "\n",
    "\n",
    "import codecs\n",
    "import pickle\n",
    "pickled = codecs.encode(pickle.dumps(model), \"base64\").decode()\n",
    "model_json={'model':pickled, 'version':version}\n",
    "#unpickled = pickle.loads(codecs.decode(model_json['model'].encode(), \"base64\"))\n",
    "model_updates_producer.produce(MODEL_UPDATES_TOPIC,value=json.dumps(model_json), key=str(uuid))\n",
    "model_updates_producer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Read Published Models\n",
    "\n",
    "\n",
    "![Alt text](./images/kafka_mlflow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "uuid=uuid.uuid1()\n",
    "group_id = f'grp-{uuid}'\n",
    "import time\n",
    "import os\n",
    "from confluent_kafka import TopicPartition,Producer,Consumer\n",
    "import certifi\n",
    "\n",
    "def get_latest_model(models:{},group_id):    \n",
    "    attempt=1\n",
    "    latest_version=0\n",
    "    model_update_consumer_conf = {\n",
    "                     'bootstrap.servers': os.environ.get('KAFKA_BOOTSTRAP_SERVERS'),\n",
    "                     'sasl.username': os.environ.get('KAFKA_USER_NAME'),\n",
    "                     'sasl.password': os.environ.get('KAFKA_PASSWORD'),\n",
    "                     'sasl.mechanism': 'PLAIN',\n",
    "                     'security.protocol': 'SASL_SSL',\n",
    "                     'ssl.ca.location': certifi.where(),\n",
    "                     'group.id': group_id,\n",
    "                     'enable.auto.commit': False,\n",
    "                     'auto.offset.reset': 'earliest'}\n",
    "    model_updates_tls = []\n",
    "    model_updates_tls.append(TopicPartition(MODEL_UPDATE_TOPIC, 0))\n",
    "\n",
    "    model_update_consumer = Consumer(model_update_consumer_conf)\n",
    "    model_update_consumer.assign(model_updates_tls)    \n",
    "    msg = model_update_consumer.poll(timeout=1.0)\n",
    "    if not msg:\n",
    "        msg = model_update_consumer.poll(timeout=1.0)    \n",
    "    while(True):\n",
    "        if(msg):\n",
    "            if msg.error():\n",
    "                if msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "                    sys.stderr.write('%% %s [%d] reached end at offset %d\\n' %\n",
    "                                    (msg.topic(), msg.partition(), msg.offset()))\n",
    "                elif msg.error():\n",
    "                    sys.stderr.write(f'Error code{msg.error().code()} \\n')\n",
    "            else:\n",
    "                model_json = json.loads(msg.value().decode(\"utf-8\"))\n",
    "                picked_str=model_json['model']\n",
    "                model_instance = pickle.loads(codecs.decode(model_json['model'].encode(), \"base64\"))\n",
    "                model_version = int(model_json['version'])\n",
    "                print(f'Retrived{model_version}')\n",
    "                models[model_version]=model_instance\n",
    "                if(model_version>latest_version):\n",
    "                    latest_version = model_version\n",
    "                model_update_consumer.commit()\n",
    "        \n",
    "            msg = model_update_consumer.poll(timeout=1.0) \n",
    "        else:\n",
    "            print('Waiting')\n",
    "            msg = model_update_consumer.poll(timeout=10.0) \n",
    "    print('Returning')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "import uuid\n",
    "uuid=uuid.uuid1()\n",
    "group_id = f'grp-{uuid}'\n",
    "models = {}\n",
    "x = threading.Thread(target=get_latest_model, args=(models,group_id))\n",
    "x.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrived2\n",
      "Retrived4\n",
      "Retrived5\n",
      "[[ 0.81215159  1.38030805 -1.18624797  0.72191769 -1.43503289]\n",
      " [-0.82008976  0.45947219 -0.52093021  0.37650602 -1.00376716]\n",
      " [ 0.29643911  1.74347419 -0.42547333  1.47592002 -1.76337106]\n",
      " [ 1.77461022  0.40495126 -0.00617309 -1.12599993 -0.13786538]\n",
      " [ 0.11452451 -0.11403792 -1.85909673 -0.93703728 -1.53429561]]\n",
      "Waiting\n",
      "Waiting\n"
     ]
    }
   ],
   "source": [
    "X = data[0][0:5]\n",
    "Y = data[1][0:5]\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Generate Features\n",
    "\n",
    "![Alt text](./images/generate_features.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%4|1677704014.589|TERMINATE|client-features-1#producer-11| [thrd:app]: Producer terminating with 10 messages (1332 bytes) still in queue or transit: use flush() to wait for outstanding message delivery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting\n"
     ]
    }
   ],
   "source": [
    "# Import dependencies\n",
    "starting_index=0\n",
    "import os\n",
    "\n",
    "features_producer = Producer(producer_conf)\n",
    "\n",
    "data = sklearn.datasets.make_classification(n_samples=100, n_classes=2,n_clusters_per_class=1, n_features=5,n_informative=2, n_redundant=0, n_repeated=0)\n",
    "X = data[0]\n",
    "Y = data[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flushing Current Message Id-211\n",
      "Flushing Current Message Id-221\n",
      "Flushing Current Message Id-231\n",
      "Flushing Current Message Id-241\n",
      "Flushing Current Message Id-251\n",
      "Flushing Current Message Id-261\n",
      "Flushing Current Message Id-271\n",
      "Flushing Current Message Id-281\n",
      "Flushing Current Message Id-291\n",
      "Final Flush Current Message Id-300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting\n",
      "waiting for more features\n",
      "Waiting\n",
      "waiting for more features\n",
      "Waiting\n",
      "waiting for more features\n",
      "Waiting\n",
      "waiting for more features\n",
      "Waiting\n",
      "waiting for more features\n",
      "Waiting\n",
      "waiting for more features\n",
      "Waiting\n",
      "waiting for more features\n",
      "Waiting\n",
      "waiting for more features\n",
      "Waiting\n",
      "waiting for more features\n",
      "Waiting\n",
      "waiting for more features\n",
      "Waiting\n",
      "waiting for more features\n",
      "Waiting\n",
      "waiting for more features\n",
      "Waiting\n",
      "waiting for more features\n",
      "Waiting\n",
      "waiting for more features\n",
      "Waiting\n",
      "waiting for more features\n",
      "Waiting\n",
      "waiting for more features\n",
      "Waiting\n",
      "waiting for more features\n",
      "Waiting\n",
      "waiting for more features\n",
      "Waiting\n",
      "waiting for more features\n",
      "Waiting\n",
      "waiting for more features\n",
      "Waiting\n",
      "waiting for more features\n",
      "Waiting\n",
      "waiting for more features\n",
      "Waiting\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import certifi\n",
    "from confluent_kafka import TopicPartition,Producer,Consumer\n",
    "import json\n",
    "\n",
    "client_id='client-features-1'\n",
    "\n",
    "producer_conf = {\n",
    "                 'bootstrap.servers': os.environ.get('KAFKA_BOOTSTRAP_SERVERS'),\n",
    "                 'sasl.username': os.environ.get('KAFKA_USER_NAME'),\n",
    "                 'sasl.password': os.environ.get('KAFKA_PASSWORD'),\n",
    "                 'sasl.mechanism': 'PLAIN',\n",
    "                 'security.protocol': 'SASL_SSL',\n",
    "                 'ssl.ca.location': certifi.where(),\n",
    "                 'client.id': client_id}\n",
    "\n",
    "\n",
    "for index in range(len(X)):    \n",
    "    starting_index = starting_index + 1        \n",
    "    x_test = X[index].tolist()\n",
    "    y_test = Y[index].tolist()\n",
    "    json.dumps({'X':x_test,'Y':str(y_test)}).encode('utf-8')\n",
    "    k_record = json.dumps({'index':starting_index, 'X':x_test,'Y':str(y_test)}).encode('utf-8')\n",
    "    features_producer.produce(FEATURES_TOPIC, value=k_record, key=str(index))\n",
    "    if index>0 and index%10==0:\n",
    "        print(f'Flushing Current Message Id-{starting_index}')\n",
    "        features_producer.flush()\n",
    "print(f'Final Flush Current Message Id-{starting_index}')\n",
    "features_producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waiting for more features\n",
      "waiting for more features\n",
      "Waiting\n",
      "waiting for more features\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwaiting for more features\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[43mfeatures_consumer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m  \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Consumer Features\n",
    "features_consumer_conf = {\n",
    "                     'bootstrap.servers': os.environ.get('KAFKA_BOOTSTRAP_SERVERS'),\n",
    "                     'sasl.username': os.environ.get('KAFKA_USER_NAME'),\n",
    "                     'sasl.password': os.environ.get('KAFKA_PASSWORD'),\n",
    "                     'sasl.mechanism': 'PLAIN',\n",
    "                     'security.protocol': 'SASL_SSL',\n",
    "                     'ssl.ca.location': certifi.where(),\n",
    "                     'group.id': 'TEST-GROUP-ID',\n",
    "                     'enable.auto.commit': True,\n",
    "                     'auto.offset.reset': 'latest'}\n",
    "features_consumer = Consumer(features_consumer_conf)\n",
    "features_consumer.subscribe([FEATURES_TOPIC])\n",
    "msg = features_consumer.poll(1)    \n",
    "while(True):\n",
    "    if msg:\n",
    "        #latest_version = max(list(models.keys()))\n",
    "        features_json = json.loads(msg.value().decode(\"utf-8\"))        \n",
    "            \n",
    "        y_hat = models[latest_version].predict([features_json['X']])[0]\n",
    "        \n",
    "            \n",
    "        features_json['MODEL_VERSION']=str(latest_version)\n",
    "        features_json['Y_HAT']=str(y_hat)\n",
    "        features_json['FEATURE_INDEX']=str(features_json['index'])\n",
    "        p_record = json.dumps(features_json).encode('utf-8')\n",
    "        predictions_producer.produce(PREDICTION_TOPIC, value=p_record, key=msg.key())\n",
    "        predictions_producer.flush()\n",
    "        msg = features_consumer.poll(1)\n",
    "    else:\n",
    "        print('waiting for more features')\n",
    "        msg = features_consumer.poll(10)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def consume_features(group_id:str):    \n",
    "    FEATURES_TOPIC=f'{model_name}-features'\n",
    "    print(FEATURES_TOPIC)\n",
    "\n",
    "    latest_version = max(list(models.keys()))\n",
    "\n",
    "    features_tls = []\n",
    "    features_tls.append(TopicPartition(FEATURES_TOPIC, 0))\n",
    "    features_tls.append(TopicPartition(FEATURES_TOPIC, 1))\n",
    "    features_tls.append(TopicPartition(FEATURES_TOPIC, 2))\n",
    "    features_tls.append(TopicPartition(FEATURES_TOPIC, 3))\n",
    "\n",
    "    #Only one model instance recieves the message (Each has the SAME consumer group)\n",
    "    features_consumer_conf = {\n",
    "                     'bootstrap.servers': os.environ.get('KAFKA_BOOTSTRAP_SERVERS'),\n",
    "                     'sasl.username': os.environ.get('KAFKA_USER_NAME'),\n",
    "                     'sasl.password': os.environ.get('KAFKA_PASSWORD'),\n",
    "                     'sasl.mechanism': 'PLAIN',\n",
    "                     'security.protocol': 'SASL_SSL',\n",
    "                     'ssl.ca.location': certifi.where(),\n",
    "                     'group.id': msg = features_consumer.poll(1)    \n",
    "    while(True):\n",
    "        if msg:\n",
    "            #latest_version = max(list(models.keys()))\n",
    "            features_json = json.loads(msg.value().decode(\"utf-8\"))        \n",
    "            \n",
    "            y_hat = models[latest_version].predict([features_json['X']])[0]\n",
    "            features_consumer.commit()\n",
    "            \n",
    "            features_json['MODEL_VERSION']=str(latest_version)\n",
    "            features_json['Y_HAT']=str(y_hat)\n",
    "            features_json['FEATURE_INDEX']=str(features_json['index'])\n",
    "            p_record = json.dumps(features_json).encode('utf-8')\n",
    "            predictions_producer.produce(PREDICTION_TOPIC, value=p_record, key=msg.key())\n",
    "            predictions_producer.flush()\n",
    "            msg = features_consumer.poll(1)    \n",
    "            \n",
    "        else:\n",
    "            print('waiting for more features')\n",
    "            msg = features_consumer.poll(10)  \n",
    "        ,\n",
    "                     'enable.auto.commit': True,\n",
    "                     'auto.offset.reset': 'latest'}\n",
    "    features_consumer = Consumer(features_consumer_conf)\n",
    "    features_consumer.subscribe([FEATURES_TOPIC])\n",
    "    #features_consumer.assign(features_tls)    \n",
    "\n",
    "    client_id='client-1'\n",
    "    producer_conf = {\n",
    "                     'bootstrap.servers': os.environ.get('KAFKA_BOOTSTRAP_SERVERS'),\n",
    "                     'sasl.username': os.environ.get('KAFKA_USER_NAME'),\n",
    "                     'sasl.password': os.environ.get('KAFKA_PASSWORD'),\n",
    "                     'sasl.mechanism': 'PLAIN',\n",
    "                     'security.protocol': 'SASL_SSL',\n",
    "                     'ssl.ca.location': certifi.where(),\n",
    "                     'client.id': client_id}\n",
    "    PREDICTION_TOPIC=f'{model_name}-predictions'\n",
    "    predictions_producer = Producer(producer_conf)\n",
    "\n",
    "    msg = None\n",
    "    msg = features_consumer.poll(1)    \n",
    "    while(True):\n",
    "        if msg:\n",
    "            #latest_version = max(list(models.keys()))\n",
    "            features_json = json.loads(msg.value().decode(\"utf-8\"))        \n",
    "            \n",
    "            y_hat = models[latest_version].predict([features_json['X']])[0]\n",
    "            features_consumer.commit()\n",
    "            \n",
    "            features_json['MODEL_VERSION']=str(latest_version)\n",
    "            features_json['Y_HAT']=str(y_hat)\n",
    "            features_json['FEATURE_INDEX']=str(features_json['index'])\n",
    "            p_record = json.dumps(features_json).encode('utf-8')\n",
    "            predictions_producer.produce(PREDICTION_TOPIC, value=p_record, key=msg.key())\n",
    "            predictions_producer.flush()\n",
    "            msg = features_consumer.poll(1)    \n",
    "            \n",
    "        else:\n",
    "            print('waiting for more features')\n",
    "            msg = features_consumer.poll(10)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KAFKA_DEMO-features\n",
      "waiting for more features\n",
      "Waiting\n",
      "waiting for more features\n",
      "Waiting\n",
      "waiting for more features\n",
      "Waiting\n",
      "waiting for more features\n",
      "Waiting\n",
      "waiting for more features\n",
      "Waiting\n",
      "waiting for more features\n",
      "Waiting\n",
      "waiting for more features\n",
      "Waiting\n",
      "waiting for more features\n",
      "Waiting\n",
      "waiting for more features\n",
      "Waiting\n",
      "waiting for more features\n",
      "Waiting\n",
      "waiting for more features\n",
      "Waiting\n",
      "waiting for more features\n",
      "Waiting\n",
      "waiting for more features\n",
      "Waiting\n",
      "waiting for more features\n",
      "Waiting\n",
      "waiting for more features\n",
      "Waiting\n",
      "waiting for more features\n",
      "Waiting\n",
      "waiting for more features\n",
      "Waiting\n",
      "waiting for more features\n",
      "Waiting\n",
      "waiting for more features\n",
      "Waiting\n",
      "waiting for more features\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "inference_group_id = f'sameer-1'\n",
    "cf = threading.Thread(target=consume_features, args=(inference_group_id,))\n",
    "cf.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CLUSTER_API_KEY']=CLUSTER_API_KEY\n",
    "os.environ['CLUSTER_API_SECRET']=CLUSTER_API_SECRET\n",
    "os.environ['BOOTSTRAP_SERVERS']='pkc-6ojv2.us-west4.gcp.confluent.cloud:9092'\n",
    "os.environ['MODEL_NAME']='example_model'\n",
    "os.environ['INFERENCE_GROUP_ID']='sameer-test-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Make REST API Calls to Model API Endpoint\n",
    "![Alt text](./images/make_rest_calls.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "```Payload to the model```\n",
    "\n",
    "For the lastest model version\n",
    "```\n",
    "{\n",
    "  \"data\": {\n",
    "    \"x\": [1,1,1,1,1]\n",
    "  }\n",
    "}\n",
    "```\n",
    "For the version 5 of the model\n",
    "```\n",
    "{\n",
    "  \"data\": {\n",
    "    \"x\": [1,1,1,1,1],\n",
    "    \"version\": 5\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kafka_model.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
