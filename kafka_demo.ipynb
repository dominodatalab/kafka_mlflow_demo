{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DO IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import random\n",
    "from confluent_kafka import TopicPartition,Producer,Consumer\n",
    "import uuid\n",
    "import certifi\n",
    "import codecs\n",
    "import os\n",
    "\n",
    "import mlflow\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas\n",
    "import pickle\n",
    "\n",
    "CLUSTER_API_KEY= os.environ['KAFKA_USER_NAME']\n",
    "CLUSTER_API_SECRET= os.environ['KAFKA_PASSWORD']\n",
    "KAFKA_BOOTSTRAP_SERVERS = os.environ['KAFKA_BOOTSTRAP_SERVERS']\n",
    "os.environ['KAFKA_USER_NAME']=CLUSTER_API_KEY\n",
    "os.environ['KAFKA_PASSWORD']=CLUSTER_API_SECRET\n",
    "os.environ['KAFKA_BOOTSTRAP_SERVERS']='pkc-6ojv2.us-west4.gcp.confluent.cloud:9092'\n",
    "model_name= os.environ['DOMINO_PROJECT_NAME']\n",
    "model_name_prefix = model_name\n",
    "uuid=uuid.uuid1()\n",
    "\n",
    "FEATURES_TOPIC=f'{model_name_prefix}-features'\n",
    "PREDICTION_TOPIC=f'{model_name_prefix}-predictions'\n",
    "MODEL_UPDATE_TOPIC=f'{model_name_prefix}-updates'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a test model and publish to MLflow Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXPERIMENT_NAME = model_name_prefix\n",
    "#EXPERIMENT_ID = mlflow.create_experiment(EXPERIMENT_NAME)\n",
    "exp = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "if not exp:\n",
    "    EXPERIMENT_ID = mlflow.create_experiment(EXPERIMENT_NAME)\n",
    "else:\n",
    "    EXPERIMENT_ID = exp.experiment_id\n",
    "EXPERIMENT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1)\n",
    "idx=0\n",
    "RUN_NAME = f\"run_{idx}\"\n",
    "with mlflow.start_run(experiment_id=EXPERIMENT_ID, run_name=RUN_NAME) as run:\n",
    "    # Retrieve run id\n",
    "    RUN_ID = run.info.run_id\n",
    "    data = sklearn.datasets.make_classification(n_samples=1000, n_classes=2,n_clusters_per_class=1, n_features=5,n_informative=2, n_redundant=0, n_repeated=0)\n",
    "    X = data[0]\n",
    "    y = data[1]\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X, y)\n",
    "    # Track parameters\n",
    "    mlflow.log_param(\"n_classes\", 2)\n",
    "    mlflow.log_param(\"n_clusters_per_class\", 1)\n",
    "    mlflow.log_param(\"n_features\", 5)\n",
    "    mlflow.log_param(\"n_informative\", 2)\n",
    "    mlflow.log_param(\"n_redundant\", 0)\n",
    "    mlflow.log_param(\"n_repeated\", 0)\n",
    "\n",
    "    # Track metrics - Value is fake for this demo\n",
    "    accuracy = 90\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    \n",
    "\n",
    "    # Track model\n",
    "    output = mlflow.sklearn.log_model(model, \"regression\",registered_model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(output.model_uri)\n",
    "client =  mlflow.tracking.MlflowClient()\n",
    "registered_model = client.get_registered_model(model_name)\n",
    "#print(registered_model)\n",
    "versions = registered_model.latest_versions\n",
    "print(versions[0])\n",
    "model_name = versions[0].name\n",
    "model_version = versions[0].version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Test Model Artifact Serialization and Deserialization\n",
    "import codecs\n",
    "import pickle\n",
    "\n",
    "pickled = codecs.encode(pickle.dumps(model), \"base64\").decode()\n",
    "model_instance = pickle.loads(codecs.decode(pickled.encode(), \"base64\"))\n",
    "print(type(model_instance))\n",
    "model_instance.predict([[1,1,1,1,1]])[0]      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Publish Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import os\n",
    "import random\n",
    "import certifi\n",
    "from confluent_kafka import TopicPartition,Producer,Consumer\n",
    "import json\n",
    "\n",
    "import uuid\n",
    "uuid=uuid.uuid1()\n",
    "version = model_version\n",
    "\n",
    "model_name = model_name\n",
    "client_id = f'{model_name}-publish'\n",
    "producer_conf = {'bootstrap.servers': os.environ.get('KAFKA_BOOTSTRAP_SERVERS'),\n",
    "                 'sasl.username': os.environ.get('KAFKA_USER_NAME'),\n",
    "                 'sasl.password': os.environ.get('KAFKA_PASSWORD'),\n",
    "                 'sasl.mechanism': 'PLAIN',\n",
    "                 'security.protocol': 'SASL_SSL',\n",
    "                 'ssl.ca.location': certifi.where(),\n",
    "                 'client.id': client_id}\n",
    "\n",
    "MODEL_UPDATES_TOPIC=f'{model_name}-updates'\n",
    "print(MODEL_UPDATES_TOPIC)\n",
    "\n",
    "model_updates_producer = Producer(producer_conf)\n",
    "\n",
    "\n",
    "import codecs\n",
    "import pickle\n",
    "pickled = codecs.encode(pickle.dumps(model), \"base64\").decode()\n",
    "model_json={'model':pickled, 'version':version}\n",
    "#unpickled = pickle.loads(codecs.decode(model_json['model'].encode(), \"base64\"))\n",
    "model_updates_producer.produce(MODEL_UPDATES_TOPIC,value=json.dumps(model_json), key=str(uuid))\n",
    "model_updates_producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Published Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "uuid=uuid.uuid1()\n",
    "group_id = f'grp-{uuid}'\n",
    "import time\n",
    "import os\n",
    "from confluent_kafka import TopicPartition,Producer,Consumer\n",
    "import certifi\n",
    "\n",
    "def get_latest_model(models:{},group_id):    \n",
    "    attempt=1\n",
    "    latest_version=0\n",
    "    model_update_consumer_conf = {\n",
    "                     'bootstrap.servers': os.environ.get('KAFKA_BOOTSTRAP_SERVERS'),\n",
    "                     'sasl.username': os.environ.get('KAFKA_USER_NAME'),\n",
    "                     'sasl.password': os.environ.get('KAFKA_PASSWORD'),\n",
    "                     'sasl.mechanism': 'PLAIN',\n",
    "                     'security.protocol': 'SASL_SSL',\n",
    "                     'ssl.ca.location': certifi.where(),\n",
    "                     'group.id': group_id,\n",
    "                     'enable.auto.commit': False,\n",
    "                     'auto.offset.reset': 'earliest'}\n",
    "    model_updates_tls = []\n",
    "    model_updates_tls.append(TopicPartition(MODEL_UPDATE_TOPIC, 0))\n",
    "\n",
    "    model_update_consumer = Consumer(model_update_consumer_conf)\n",
    "    model_update_consumer.assign(model_updates_tls)    \n",
    "    msg = model_update_consumer.poll(timeout=1.0)\n",
    "    if not msg:\n",
    "        msg = model_update_consumer.poll(timeout=1.0)    \n",
    "    while(True):\n",
    "        if(msg):\n",
    "            if msg.error():\n",
    "                if msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "                    sys.stderr.write('%% %s [%d] reached end at offset %d\\n' %\n",
    "                                    (msg.topic(), msg.partition(), msg.offset()))\n",
    "                elif msg.error():\n",
    "                    sys.stderr.write(f'Error code{msg.error().code()} \\n')\n",
    "            else:\n",
    "                model_json = json.loads(msg.value().decode(\"utf-8\"))\n",
    "                picked_str=model_json['model']\n",
    "                model_instance = pickle.loads(codecs.decode(model_json['model'].encode(), \"base64\"))\n",
    "                model_version = int(model_json['version'])\n",
    "                print(f'Retrived{model_version}')\n",
    "                models[model_version]=model_instance\n",
    "                if(model_version>latest_version):\n",
    "                    latest_version = model_version\n",
    "                model_update_consumer.commit()\n",
    "        \n",
    "            msg = model_update_consumer.poll(timeout=1.0) \n",
    "        else:\n",
    "            print('Waiting')\n",
    "            msg = model_update_consumer.poll(timeout=10.0) \n",
    "    print('Returning')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "import uuid\n",
    "uuid=uuid.uuid1()\n",
    "group_id = f'grp-{uuid}'\n",
    "models = {}\n",
    "x = threading.Thread(target=get_latest_model, args=(models,group_id))\n",
    "x.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = data[0][0:5]\n",
    "Y = data[1][0:5]\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Generate Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import os\n",
    "import random\n",
    "import certifi\n",
    "from confluent_kafka import TopicPartition,Producer,Consumer\n",
    "import json\n",
    "model_name='example_model'\n",
    "client_id='client-features-1'\n",
    "producer_conf = {\n",
    "                 'bootstrap.servers': os.environ.get('KAFKA_BOOTSTRAP_SERVERS'),\n",
    "                 'sasl.username': os.environ.get('KAFKA_USER_NAME'),\n",
    "                 'sasl.password': os.environ.get('KAFKA_PASSWORD'),\n",
    "                 'sasl.mechanism': 'PLAIN',\n",
    "                 'security.protocol': 'SASL_SSL',\n",
    "                 'ssl.ca.location': certifi.where(),\n",
    "                 'client.id': client_id}\n",
    "\n",
    "\n",
    "features_producer = Producer(producer_conf)\n",
    "X = data[0]\n",
    "Y = data[1]\n",
    "print(X)\n",
    "for index in range(5):\n",
    "    x_test = X[index].tolist()\n",
    "    y_test = Y[index].tolist()\n",
    "    json.dumps({'X':x_test,'Y':str(y_real)}).encode('utf-8')\n",
    "    k_record = json.dumps({'X':x_test,'Y':str(y_real)}).encode('utf-8')\n",
    "    features_producer.produce(FEATURES_TOPIC, value=k_record, key=str(index))\n",
    "    if index>0 and index%1000==0:\n",
    "        print('Flushing')\n",
    "        features_producer.flush()\n",
    "print('Flushing')\n",
    "features_producer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def consume_features(group_id:str):    \n",
    "\n",
    "    model_name='example_model'\n",
    "    FEATURES_TOPIC=f'{model_name}-features'\n",
    "    print(FEATURES_TOPIC)\n",
    "\n",
    "    latest_version = max(list(models.keys()))\n",
    "\n",
    "    features_tls = []\n",
    "    features_tls.append(TopicPartition(FEATURES_TOPIC, 0))\n",
    "    features_tls.append(TopicPartition(FEATURES_TOPIC, 1))\n",
    "    features_tls.append(TopicPartition(FEATURES_TOPIC, 2))\n",
    "    features_tls.append(TopicPartition(FEATURES_TOPIC, 3))\n",
    "\n",
    "    #Only one model instance recieves the message (Each has the SAME consumer group)\n",
    "    features_consumer_conf = {\n",
    "                     'bootstrap.servers': os.environ.get('KAFKA_BOOTSTRAP_SERVERS'),\n",
    "                     'sasl.username': os.environ.get('KAFKA_USER_NAME'),\n",
    "                     'sasl.password': os.environ.get('KAFKA_PASSWORD'),\n",
    "                     'sasl.mechanism': 'PLAIN',\n",
    "                     'security.protocol': 'SASL_SSL',\n",
    "                     'ssl.ca.location': certifi.where(),\n",
    "                     'group.id': group_id,\n",
    "                     'enable.auto.commit': True,\n",
    "                     'auto.offset.reset': 'latest'}\n",
    "    features_consumer = Consumer(features_consumer_conf)\n",
    "    #features_consumer.subscribe([FEATURES_TOPIC])\n",
    "    features_consumer.assign(features_tls)    \n",
    "\n",
    "    client_id='client-1'\n",
    "    producer_conf = {\n",
    "                     'bootstrap.servers': os.environ.get('KAFKA_BOOTSTRAP_SERVERS'),\n",
    "                     'sasl.username': os.environ.get('KAFKA_USER_NAME'),\n",
    "                     'sasl.password': os.environ.get('KAFKA_PASSWORD'),\n",
    "                     'sasl.mechanism': 'PLAIN',\n",
    "                     'security.protocol': 'SASL_SSL',\n",
    "                     'ssl.ca.location': certifi.where(),\n",
    "                     'client.id': client_id}\n",
    "    PREDICTION_TOPIC=f'{model_name}-predictions'\n",
    "    predictions_producer = Producer(producer_conf)\n",
    "\n",
    "    msg = None\n",
    "    msg = features_consumer.poll(1)    \n",
    "    while(True):\n",
    "        if msg:\n",
    "            #latest_version = max(list(models.keys()))\n",
    "            features_json = json.loads(msg.value().decode(\"utf-8\"))        \n",
    "            \n",
    "            y_hat = models[latest_version].predict([features_json['X']])[0]\n",
    "            features_consumer.commit()\n",
    "            features_json['Y_HAT']=str(y_hat)\n",
    "            features_json['MODEL_VERSION']=str(latest_version)\n",
    "            p_record = json.dumps(features_json).encode('utf-8')\n",
    "            predictions_producer.produce(PREDICTION_TOPIC, value=p_record, key=msg.key())\n",
    "            predictions_producer.flush()\n",
    "            msg = features_consumer.poll(1)    \n",
    "            \n",
    "        else:\n",
    "            print('waiting for more features')\n",
    "            msg = features_consumer.poll(10)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_group_id = f'sameer-1'\n",
    "cf = threading.Thread(target=consume_features, args=(inference_group_id,))\n",
    "cf.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cf.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CLUSTER_API_KEY']=CLUSTER_API_KEY\n",
    "os.environ['CLUSTER_API_SECRET']=CLUSTER_API_SECRET\n",
    "os.environ['BOOTSTRAP_SERVERS']='pkc-6ojv2.us-west4.gcp.confluent.cloud:9092'\n",
    "os.environ['MODEL_NAME']='example_model'\n",
    "os.environ['INFERENCE_GROUP_ID']='sameer-test-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kafka_model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kafka_model.predict(x=[1,1,1,1,1],version=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kafka_model.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
