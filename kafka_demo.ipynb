{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DO IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "str expected, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKAFKA_PASSWORD\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mCLUSTER_API_SECRET\n\u001b[1;32m     21\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKAFKA_BOOTSTRAP_SERVERS\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpkc-6ojv2.us-west4.gcp.confluent.cloud:9092\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 22\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mKAFKA_FEATURES_TOPIC_PARTITION_RANGE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0-9\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;66;03m#Inclusive of 0 and 9\u001b[39;00m\n\u001b[1;32m     23\u001b[0m model_name\u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDOMINO_PROJECT_NAME\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     24\u001b[0m model_name_prefix \u001b[38;5;241m=\u001b[39m model_name\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/os.py:684\u001b[0m, in \u001b[0;36m_Environ.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__setitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value):\n\u001b[1;32m    683\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencodekey(key)\n\u001b[0;32m--> 684\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencodevalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    685\u001b[0m     putenv(key, value)\n\u001b[1;32m    686\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data[key] \u001b[38;5;241m=\u001b[39m value\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/os.py:756\u001b[0m, in \u001b[0;36m_createenviron.<locals>.encode\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    754\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(value):\n\u001b[1;32m    755\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 756\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstr expected, not \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\u001b[38;5;241m.\u001b[39mencode(encoding, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogateescape\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: str expected, not list"
     ]
    }
   ],
   "source": [
    "# Import dependencies\n",
    "import random\n",
    "from confluent_kafka import TopicPartition,Producer,Consumer\n",
    "import uuid\n",
    "import certifi\n",
    "import codecs\n",
    "import os\n",
    "\n",
    "import mlflow\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas\n",
    "import pickle\n",
    "\n",
    "CLUSTER_API_KEY= os.environ['KAFKA_USER_NAME']\n",
    "CLUSTER_API_SECRET= os.environ['KAFKA_PASSWORD']\n",
    "KAFKA_BOOTSTRAP_SERVERS = os.environ['KAFKA_BOOTSTRAP_SERVERS']\n",
    "os.environ['KAFKA_USER_NAME']=CLUSTER_API_KEY\n",
    "os.environ['KAFKA_PASSWORD']=CLUSTER_API_SECRET\n",
    "os.environ['KAFKA_BOOTSTRAP_SERVERS']='pkc-6ojv2.us-west4.gcp.confluent.cloud:9092'\n",
    "os.environ['KAFKA_FEATURES_TOPIC_PARTITION_RANGE']= ['0-4'] #Inclusive of 0 and 9\n",
    "model_name= os.environ['DOMINO_PROJECT_NAME']\n",
    "model_name_prefix = model_name\n",
    "uuid=uuid.uuid1()\n",
    "\n",
    "FEATURES_TOPIC=f'{model_name_prefix}-features'\n",
    "PREDICTION_TOPIC=f'{model_name_prefix}-predictions'\n",
    "MODEL_UPDATE_TOPIC=f'{model_name_prefix}-updates'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a test model and publish to MLflow Model Registry\n",
    "\n",
    "![Alt text](./images/publish_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## What is a Domino Model API Endpoint in the Streaming Context\n",
    "\n",
    "A Domino API Endpoint is a multi-relica REST Endpoint which also acts as a Kafka Consumer and Producer. It consumes features and model version updates \n",
    "from separate Kafka topics and generates predictions and writes them to the prediction topic. This mode of predictions operates on the push mechanism \n",
    "where the incoming features are passed to the latest model version and generates new predictions.\n",
    "\n",
    "It also provides an API Endpoint to invoke interactively. By default it uses the latest model but can also be used to generate perdiction of any previous model version.\n",
    "\n",
    "A Model Endpoint is configured with-\n",
    "1. A Python file containing the model code\n",
    "2. A function call to be invoked when invoking the model via REST call\n",
    "3. Environment variables which contain all the connectivity details for Kafka (These can be be also injected in more secure manner such as Vault or \n",
    "                                                                               IRSA or Workload Idenities)\n",
    "4. A Kafka Consumer or Producer that is continuously listening.\n",
    "\n",
    "\n",
    "The Kafka Consumers for features across all model api instances will use the same consumer group id. However the Kafka Consumer for the Model Updates topic will use\n",
    "a unique group id per instance.\n",
    "\n",
    "\n",
    "![Alt text](./images/make_rest_calls.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = model_name_prefix\n",
    "#EXPERIMENT_ID = mlflow.create_experiment(EXPERIMENT_NAME)\n",
    "exp = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "if not exp:\n",
    "    EXPERIMENT_ID = mlflow.create_experiment(EXPERIMENT_NAME)\n",
    "else:\n",
    "    EXPERIMENT_ID = exp.experiment_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1)\n",
    "idx=0\n",
    "RUN_NAME = f\"run_{idx}\"\n",
    "with mlflow.start_run(experiment_id=EXPERIMENT_ID, run_name=RUN_NAME) as run:\n",
    "    # Retrieve run id\n",
    "    RUN_ID = run.info.run_id\n",
    "    data = sklearn.datasets.make_classification(n_samples=1000, n_classes=2,n_clusters_per_class=1, n_features=5,n_informative=2, n_redundant=0, n_repeated=0)\n",
    "    X = data[0]\n",
    "    y = data[1]\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X, y)\n",
    "    # Track parameters\n",
    "    mlflow.log_param(\"n_classes\", 2)\n",
    "    mlflow.log_param(\"n_clusters_per_class\", 1)\n",
    "    mlflow.log_param(\"n_features\", 5)\n",
    "    mlflow.log_param(\"n_informative\", 2)\n",
    "    mlflow.log_param(\"n_redundant\", 0)\n",
    "    mlflow.log_param(\"n_repeated\", 0)\n",
    "\n",
    "    # Track metrics - Value is fake for this demo\n",
    "    accuracy = 90\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    \n",
    "\n",
    "    # Track model\n",
    "    output = mlflow.sklearn.log_model(model, \"regression\",registered_model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(output.model_uri)\n",
    "client =  mlflow.tracking.MlflowClient()\n",
    "registered_model = client.get_registered_model(model_name)\n",
    "#print(registered_model)\n",
    "versions = registered_model.latest_versions\n",
    "print(versions[0])\n",
    "model_name = versions[0].name\n",
    "model_version = versions[0].version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.transition_model_version_stage(\n",
    "  name=model_name,\n",
    "  version=model_version,\n",
    "  stage='Staging',\n",
    ")\n",
    "#client.get_registered_model(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Test Model Artifact Serialization and Deserialization\n",
    "import codecs\n",
    "import pickle\n",
    "\n",
    "pickled = codecs.encode(pickle.dumps(model), \"base64\").decode()\n",
    "model_instance = pickle.loads(codecs.decode(pickled.encode(), \"base64\"))\n",
    "print(type(model_instance))\n",
    "model_instance.predict([[1,1,1,1,1]])[0]      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Publish Serialized Model To Kafka Topic\n",
    "\n",
    "**All Domino Model API Instances will have the same Kafka Consumer Group Id to ensure that messages are processed mostly once, and idempotently**\n",
    "\n",
    "![Alt text](./images/kafka_mlflow.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import os\n",
    "import random\n",
    "import certifi\n",
    "from confluent_kafka import TopicPartition,Producer,Consumer\n",
    "import json\n",
    "\n",
    "import uuid\n",
    "uuid=uuid.uuid1()\n",
    "version = model_version\n",
    "\n",
    "model_name = model_name\n",
    "client_id = f'{model_name}-publish'\n",
    "producer_conf = {'bootstrap.servers': os.environ.get('KAFKA_BOOTSTRAP_SERVERS'),\n",
    "                 'sasl.username': os.environ.get('KAFKA_USER_NAME'),\n",
    "                 'sasl.password': os.environ.get('KAFKA_PASSWORD'),\n",
    "                 'sasl.mechanism': 'PLAIN',\n",
    "                 'security.protocol': 'SASL_SSL',\n",
    "                 'ssl.ca.location': certifi.where(),\n",
    "                 'client.id': client_id}\n",
    "\n",
    "MODEL_UPDATES_TOPIC=f'{model_name}-updates'\n",
    "print(MODEL_UPDATES_TOPIC)\n",
    "\n",
    "model_updates_producer = Producer(producer_conf)\n",
    "\n",
    "\n",
    "import codecs\n",
    "import pickle\n",
    "pickled = codecs.encode(pickle.dumps(model), \"base64\").decode()\n",
    "model_json={'model':pickled, 'version':version}\n",
    "#unpickled = pickle.loads(codecs.decode(model_json['model'].encode(), \"base64\"))\n",
    "model_updates_producer.produce(MODEL_UPDATES_TOPIC,value=json.dumps(model_json), key=str(uuid))\n",
    "model_updates_producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read Published Models\n",
    "\n",
    "\n",
    "![Alt text](./images/kafka_mlflow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "uuid=uuid.uuid1()\n",
    "group_id = f'grp-{uuid}'\n",
    "import time\n",
    "import os\n",
    "from confluent_kafka import TopicPartition,Producer,Consumer\n",
    "import certifi\n",
    "\n",
    "def get_latest_model(models:{},group_id):    \n",
    "    attempt=1\n",
    "    latest_version=0\n",
    "    model_update_consumer_conf = {\n",
    "                     'bootstrap.servers': os.environ.get('KAFKA_BOOTSTRAP_SERVERS'),\n",
    "                     'sasl.username': os.environ.get('KAFKA_USER_NAME'),\n",
    "                     'sasl.password': os.environ.get('KAFKA_PASSWORD'),\n",
    "                     'sasl.mechanism': 'PLAIN',\n",
    "                     'security.protocol': 'SASL_SSL',\n",
    "                     'ssl.ca.location': certifi.where(),\n",
    "                     'group.id': group_id,\n",
    "                     'enable.auto.commit': False,\n",
    "                     'auto.offset.reset': 'earliest'}\n",
    "    model_updates_tls = []\n",
    "    model_updates_tls.append(TopicPartition(MODEL_UPDATE_TOPIC, 0))\n",
    "\n",
    "    model_update_consumer = Consumer(model_update_consumer_conf)\n",
    "    model_update_consumer.assign(model_updates_tls)    \n",
    "    msg = model_update_consumer.poll(timeout=1.0)\n",
    "    if not msg:\n",
    "        msg = model_update_consumer.poll(timeout=1.0)    \n",
    "    while(True):\n",
    "        if(msg):\n",
    "            if msg.error():\n",
    "                if msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "                    sys.stderr.write('%% %s [%d] reached end at offset %d\\n' %\n",
    "                                    (msg.topic(), msg.partition(), msg.offset()))\n",
    "                elif msg.error():\n",
    "                    sys.stderr.write(f'Error code{msg.error().code()} \\n')\n",
    "            else:\n",
    "                model_json = json.loads(msg.value().decode(\"utf-8\"))\n",
    "                picked_str=model_json['model']\n",
    "                model_instance = pickle.loads(codecs.decode(model_json['model'].encode(), \"base64\"))\n",
    "                model_version = int(model_json['version'])\n",
    "                print(f'Retrived{model_version}')\n",
    "                models[model_version]=model_instance\n",
    "                if(model_version>latest_version):\n",
    "                    latest_version = model_version\n",
    "                model_update_consumer.commit()\n",
    "        \n",
    "            msg = model_update_consumer.poll(timeout=1.0) \n",
    "        else:\n",
    "            print('Waiting')\n",
    "            msg = model_update_consumer.poll(timeout=10.0) \n",
    "    print('Returning')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "import uuid\n",
    "uuid=uuid.uuid1()\n",
    "group_id = f'grp-{uuid}'\n",
    "models = {}\n",
    "x = threading.Thread(target=get_latest_model, args=(models,group_id))\n",
    "x.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = data[0][0:5]\n",
    "Y = data[1][0:5]\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Generate Features\n",
    "\n",
    "![Alt text](./images/generate_features.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "starting_index=0\n",
    "import os\n",
    "import random\n",
    "import certifi\n",
    "from confluent_kafka import TopicPartition,Producer,Consumer\n",
    "import json\n",
    "model_name='example_model'\n",
    "client_id='client-features-1'\n",
    "producer_conf = {\n",
    "                 'bootstrap.servers': os.environ.get('KAFKA_BOOTSTRAP_SERVERS'),\n",
    "                 'sasl.username': os.environ.get('KAFKA_USER_NAME'),\n",
    "                 'sasl.password': os.environ.get('KAFKA_PASSWORD'),\n",
    "                 'sasl.mechanism': 'PLAIN',\n",
    "                 'security.protocol': 'SASL_SSL',\n",
    "                 'ssl.ca.location': certifi.where(),\n",
    "                 'client.id': client_id}\n",
    "\n",
    "\n",
    "features_producer = Producer(producer_conf)\n",
    "X = data[0]\n",
    "Y = data[1]\n",
    "print(X)\n",
    "for index in range(5):\n",
    "    starting_index = starting_index + index\n",
    "    x_test = X[index].tolist()\n",
    "    y_test = Y[index].tolist()\n",
    "    json.dumps({'X':x_test,'Y':str(y_test)}).encode('utf-8')\n",
    "    k_record = json.dumps({'index':starting_index, 'X':x_test,'Y':str(y_test)}).encode('utf-8')\n",
    "    features_producer.produce(FEATURES_TOPIC, value=k_record, key=str(index))\n",
    "    if index>0 and index%1000==0:\n",
    "        print('Flushing')\n",
    "        features_producer.flush()\n",
    "print('Flushing')\n",
    "features_producer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def consume_features(group_id:str):    \n",
    "\n",
    "    model_name='example_model'\n",
    "    FEATURES_TOPIC=f'{model_name}-features'\n",
    "    print(FEATURES_TOPIC)\n",
    "\n",
    "    latest_version = max(list(models.keys()))\n",
    "\n",
    "    features_tls = []\n",
    "    features_tls.append(TopicPartition(FEATURES_TOPIC, 0))\n",
    "    features_tls.append(TopicPartition(FEATURES_TOPIC, 1))\n",
    "    features_tls.append(TopicPartition(FEATURES_TOPIC, 2))\n",
    "    features_tls.append(TopicPartition(FEATURES_TOPIC, 3))\n",
    "\n",
    "    #Only one model instance recieves the message (Each has the SAME consumer group)\n",
    "    features_consumer_conf = {\n",
    "                     'bootstrap.servers': os.environ.get('KAFKA_BOOTSTRAP_SERVERS'),\n",
    "                     'sasl.username': os.environ.get('KAFKA_USER_NAME'),\n",
    "                     'sasl.password': os.environ.get('KAFKA_PASSWORD'),\n",
    "                     'sasl.mechanism': 'PLAIN',\n",
    "                     'security.protocol': 'SASL_SSL',\n",
    "                     'ssl.ca.location': certifi.where(),\n",
    "                     'group.id': group_id,\n",
    "                     'enable.auto.commit': True,\n",
    "                     'auto.offset.reset': 'latest'}\n",
    "    features_consumer = Consumer(features_consumer_conf)\n",
    "    #features_consumer.subscribe([FEATURES_TOPIC])\n",
    "    features_consumer.assign(features_tls)    \n",
    "\n",
    "    client_id='client-1'\n",
    "    producer_conf = {\n",
    "                     'bootstrap.servers': os.environ.get('KAFKA_BOOTSTRAP_SERVERS'),\n",
    "                     'sasl.username': os.environ.get('KAFKA_USER_NAME'),\n",
    "                     'sasl.password': os.environ.get('KAFKA_PASSWORD'),\n",
    "                     'sasl.mechanism': 'PLAIN',\n",
    "                     'security.protocol': 'SASL_SSL',\n",
    "                     'ssl.ca.location': certifi.where(),\n",
    "                     'client.id': client_id}\n",
    "    PREDICTION_TOPIC=f'{model_name}-predictions'\n",
    "    predictions_producer = Producer(producer_conf)\n",
    "\n",
    "    msg = None\n",
    "    msg = features_consumer.poll(1)    \n",
    "    while(True):\n",
    "        if msg:\n",
    "            #latest_version = max(list(models.keys()))\n",
    "            features_json = json.loads(msg.value().decode(\"utf-8\"))        \n",
    "            \n",
    "            y_hat = models[latest_version].predict([features_json['X']])[0]\n",
    "            features_consumer.commit()\n",
    "            \n",
    "            features_json['MODEL_VERSION']=str(latest_version)\n",
    "            features_json['Y_HAT']=str(y_hat)\n",
    "            features_json['FEATURE_INDEX']=str(features_json['index'])\n",
    "            p_record = json.dumps(features_json).encode('utf-8')\n",
    "            predictions_producer.produce(PREDICTION_TOPIC, value=p_record, key=msg.key())\n",
    "            predictions_producer.flush()\n",
    "            msg = features_consumer.poll(1)    \n",
    "            \n",
    "        else:\n",
    "            print('waiting for more features')\n",
    "            msg = features_consumer.poll(10)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_group_id = f'sameer-1'\n",
    "cf = threading.Thread(target=consume_features, args=(inference_group_id,))\n",
    "cf.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CLUSTER_API_KEY']=CLUSTER_API_KEY\n",
    "os.environ['CLUSTER_API_SECRET']=CLUSTER_API_SECRET\n",
    "os.environ['BOOTSTRAP_SERVERS']='pkc-6ojv2.us-west4.gcp.confluent.cloud:9092'\n",
    "os.environ['MODEL_NAME']='example_model'\n",
    "os.environ['INFERENCE_GROUP_ID']='sameer-test-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Make REST API Calls to Model API Endpoint\n",
    "![Alt text](./images/make_rest_calls.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "import kafka_model \n",
    "\n",
    "kafka_model.predict(x=[1,1,1,1,1],version=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kafka_model.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
